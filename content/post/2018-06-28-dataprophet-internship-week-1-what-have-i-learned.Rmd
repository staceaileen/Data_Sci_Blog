---
title: 'Data Science Internship: Week 1 - What Have I Learned?'
author: 'Stacy Kam'
date: '2018-06-28'
slug: internship-week-1-what-have-i-learned
categories: [internship, python]
tags: [python, internship, neural networks, keras, cnns, tensorflow, github]
---

# The Why: 

Started my Data Science internship at a data science consulting company located in Cape Town, South Africa. In these series of posts I'll be posting topics, link resources, and intriguing things I run up to during the internship. 

## Virtual Environments

We learned how to create our in python virtual environment through pyenv instead of using Conda as we were used to during class. The difference is that Conda will include all the packages when you're deploying the program while a virtual environment will not come with all the extra unnecessary packages. 

### Problems We Ran Into:
- After installing our virtual environment following the code: `virtualenv {name_here}` and the activating it by `source {name_here}/bin/activate` we uploaded all the neccesary packages through the usual `pip install` but we were running into trouble when trying to `import matplotlib.pyplot as plt` and found out a good resrouce for that problem [here](https://matplotlib.org/faq/osx_framework.html#virtualenv), so we ended up creating a whole new environment using `venv` instead of `virtualenv`. 


## Sloth

We had to download a software called Sloth in order to create `.yaml` files containing annotations used to image recognition. 

### Problems We Ran Into:
- Apparently Sloth doesn't run on the current version of PyQt which is version 5, so we cracked our heads trying to uninstall version 5 to install version 4 instead but we found another way around it. We simply found a [Sloth that worked with version 5](https://github.com/thuyen/sloth/tree/qt5) and it fixed all our problems. 

## New Github Skills

We learned how to create new branches within repos and update the branches by merging our edited/new content. For good resource link on the topic click [here](https://github.com/Kunena/Kunena-Forum/wiki/Create-a-new-branch-with-git-and-manage-branches).

## Activation Functions in CNNs

We also started working with CNNs and our main struggle has been selecting the activation function. An activation function is what decides if a neuron should be "fired" or not, hence the name ACTIV(E)ation function. We started out with the `tanh` function which returns an output in betweet -1 and 1, then changed to the `selu` function returning outputs between 0 and infinity and finally choosing the `sigmoid` function that returns outputs from 0 to 1. We chose that one since we want our output to be in between a set range of positive numbers so having our input scaled from 0 to 1 and later multiplying it by our max range we get our output in set range. 

For more information about activation functions click [here](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0).

## Tensorflow and SSH errors

We ran into problems when trying to run our code in the SSH, the error kept saying that tensorflow couldn't start a session and it had some relation to allocating memory. We fixed it by typing `export CUDA_VISIBLE_DEVICES=‘’` on the terminal and it worked!

## Becoming a VIM expert

One of my goals this internship is to be an expert at coding with VIM, like badasses do (ehem, Sean "everything is sick" the TA). So some of the basics are:
- vi <filename> — to open
- i — to edit text
- dd — on esc mode to delete whole line
- :wq — on esc mode to exit vim

